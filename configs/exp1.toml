    seed = 42
    project_name = "nanopt-llama-3.2-1b-pretrain"
    learning_rate = 3e-4
    warmup_steps = 1000
    max_steps = 10000
    per_device_batch_size = 16
    batch_size = 16
    tokenizer_name = "meta-llama/Llama-3.2-1B"
    max_length = 4096
    num_workers = 10
    dataset_path = "data/fineweb-edu"
