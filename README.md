# NanoPT - Pretraining for ~1B parameter models

This repository is a place for me to experiment with pretraining of small (<= ~3B) language models.

This repository assumes you have access to a [modal](https://modal.com/) account.

```bash
`modal run -m scripts.01_create_data_volume::main`
```